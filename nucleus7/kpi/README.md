KPI Evaluation
==============

- [Main concepts](#concepts)
- [KPIPlugin](#kpiplugin)
- [KPIAccumulator](#kpiaccumulator)
- [KPI saving](#saving)
- [KPI caching](#caching)
- [Training](#training)


## Main concepts <a name="concepts"></a>

`KPI` (Key Performance Index) evaluation consists of following components:

* `KPIPlugin` - main part of the KPI evaluation; it calculates the
(intermediate) kpi values on inputs sample-wise, e.g. on each iteration 
* `KPIAccumulator` - accumulates the results from `KPIPlugins` and then combines
them when the signal comes; signal is the input on key `evaluate` to
accumulator. 
* `KPISaver` - used to save the KPI inside of `KPIPlugin` and `KPIAccumulator`
* `KPICacher` - used to cache the intermediate results; it is useful in case
if you don't want to recalculate the KPI values again. 

`KPIPlugin` is a `Nucleotide` and `KPIAccumulator` is a particular case of 
`KPIPlugin` and so can be combined using
common nucleus7 structure - using incoming and generated keys and
incoming_keys_mapping.
To save the results, you need to pass savers to `KPIPlugin` constructor
(the same with cachers). It can be done using configs.
This combination is done inside of `KPIEvaluator`, which
is an instance of `GeneHandler`. So it makes it straightforward to configure and
to create your own evaluation!

## KPIPlugin <a name="kpiplugin"></a>


`KPIPlugin` has a goal to calculate the (intermediate) KPI values sample-wise
and has the same structure as other nucleotides:

```python
import nucleus7 as nc7

class NewKPIPlugin(nc7.kpi.KPIPlugin):
    incoming_keys = [
        "input1",
        "input2",
    ]
    generated_keys = [
        "kpi1",
        "kpi2",
    ]

    def process(self, input1, input2):
        ...
        output = {'kpi1': kpi1, "kpi2": kpi2}
        return output
```

So the main method to override is `process`, which takes kwargs with keys as
incoming_keys and results to dict with generated_keys as keys and corresponding
kpi values. The only difference is that the inputs here are provided as
sample inputs and **NOT BATCHES**, so first dimension is already data dimension
and not batch.

## KPIAccumulator <a name="kpiaccumulator"></a>

After some values are calculated on all samples, we need to accumulate them and
then combine to real KPI values, e.g. using mean values. For these purposes
the `KPIAccumulator```

```python
import nucleus7 as nc7

class NewKPIAccumulator(nc7.kpi.KPIAccumulator):
    incoming_keys = [
        "input1",
        "input2",
    ]
    generated_keys = [
        "kpi1",
        "kpi2",
    ]

    def accumulate(self, **inputs):
        for each_key, each_value in inputs.items():
            self._accumulated_states.setdefault(each_key, [])
            self._accumulated_states[each_key].append(each_value)

    def process(self, input1, input2):
        ...
        output = {'kpi1': kpi1, "kpi2": kpi2}
        return output

    def evaluate_on_sample(self, *, evaluate=None, accumulate=True,
                           prefix = None, **inputs) -> dict:
        ...

```

So when the inputs are coming to `KPIAccumulator`, they are first accumulated
to inner state. After, when passed `evaluate` kwarg evaluates to True, it will
pass the accumulated state to `self.process`. So you need to override this
method to process the inputs. Inputs is the dict with incoming_keys keys and
values as list with each item in format passed to accumulate method.  

## KPI saving <a name="saving"></a>

To save the KPI, you need to pass the saver to `KPIPlugin` / `KPIAccumulator`
constructor. The saver should inherit from `KPISaver`:

```python
import nucleus7 as nc7

class NewKPISaver(nc7.kpi.KPISaver):
    @property
    def save_target(self):
        return self._save_target

    def save(self, name: str, values: dict):
        ...
```

So `save_target` can be set using constructor or it will be set by Coordinator
as results directory. But it may be also not a directory, but a address etc.
During training, the Saver to tensorboard (as tensorflow summaries) will be
automatically generated by Trainer. But if you want to have the values logged
to `mlflow`, you need to add the `MlflowKPILogger` to your configs. 

## KPI caching <a name="caching"></a>

You can save some time on evaluation of the same KPI if you cache it. For it
you can use `KPICacher` objects:

```python
import nucleus7 as nc7

class NewKPICacher(nc7.kpi.KPICacher):
    @property
    def cache_target(self):
        return self._cache_target

    def calculate_hash(self, inputs, cache_prefix = None):
        inputs_hash = ...
        self._current_hash = inputs_hash

    def cache(self, values):
        ...

    def restore(self):
        ...
```

To cache the values, you first need to override `calculate_hash` method.
Afterwards you can cathe the calculated values using overridden `cache` method
and restore with overridden `restore`. It has the `cache_target` attribute,
which can be set manually or will be set by Coordinator
(same as for `KPISaver`). `cache_prefix` is the key that specifies this set of
inputs. 

## Training <a name="training"></a>

During the training, the `KPIEvaluator` will be converted to
`CoordinatorCallback` and then to `SessionRunHook`. This will allow to execute
it using `Estimator API`. Also internal states of all `KPIAccumulators` will
be cleared each epoch and the evaluation will be triggered on last sample in
last iteration for `KPIAccumulators` automatically (this is also the case with
inference / kpi evaluation).
