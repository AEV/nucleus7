#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# ==============================================================================
# Copyright (c) 2019 Audi Electronics Venture GmbH. All rights reserved.
#
# This Source Code Form is subject to the terms of the Mozilla
# Public License, v. 2.0. If a copy of the MPL was not distributed
# with this file, You can obtain one at https://mozilla.org/MPL/2.0/.
# ==============================================================================
#
# pylint: disable=anomalous-backslash-in-string
# backslashes are needed to show the command since it is multi line

"""
Main script for the prediction of trained neural networks

Example
-------
.. code-block:: bash

    $ nc7-infer \
    $ path/to/project/dir \
    $ [--run_name name of the run] \
    $ [--additional_configs_path path/to/additional/configs] \
    $ [--saved_model 1554] \
    $ # is name of the directory inside of saved_models folder \
    $ [--checkpoint checkpoint_fname] \
    $ # only if saved_model not provided \
    $ # and is relative to project_dir/checkpoints \
    $ [--number_of_shards 4] \ # split data to 4 chunks
    $ [--shard_index 2] \ # and infer second chunk
    $ [--batch_size 10] \
    $ [--use_single_process] \
    $ [--use_tensorrt]

Raises
------
ValueError
    if more than 1 GPU is visible

"""
# pylint: enable=anomalous-backslash-in-string

import argparse
import os

import matplotlib
matplotlib.use('Agg')

# pylint: disable=wrong-import-position
# matplotlib should be imported before to set the backend
from nucleus7.builders.runs_builder import build_infer
from nucleus7.core.project_dirs import read_inference_configs_from_directories
from nucleus7.utils import cli_utils
from nucleus7.utils import tf_utils
# pylint: enable=wrong-import-position


def construct_parser():
    """
    Construct the parser
    """
    description = (
        "Main script to start inference.\n\n"
        "Project directory should have inference/configs folder inside with "
        "following tree:\n"
        "    * (optional) configs_main.json - main config which may contain \n"
        "all other configurations\n"
        "    * datafeeder.json - data feeder configuration\n"
        "    * (optional) inferer.json - inferer configuration\n"
        "    * (optional) callbacks/{callback1..n}.json - files with "
        "arbitrary names with callbacks\n"
        "    * kpis/{kpi_plugin_or_accumulators1..n}.json - files "
        "with arbitrary names with kpi plugins or accumulators\n"
        "This is also possible to provide the additional config directory \n"
        "with same structure, which will update the configs from here"
    )
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('project_dir', type=str,
                        help='project directory to use')
    parser.add_argument('-r', '--run_name', type=str, required=False,
                        help='name of inference run')
    parser.add_argument("-a", '--additional_configs_path', required=False,
                        type=str, help='additional directory with configs')
    parser.add_argument('-b', '--batch_size', required=False, type=int,
                        help='batch size')
    model_load_parser = parser.add_mutually_exclusive_group(required=False)
    model_load_parser.add_argument(
        "-s", '--saved_model_tag', required=False,
        type=str, help='tag for saved model, e.g. folder name relative to '
                       'saved_models subdirectory')
    model_load_parser.add_argument(
        "-w", '--checkpoint_file_name', required=False,
        type=str, help='file name for checkpoint relative to '
                       'checkpoints subdirectory (graph_inference.meta must '
                       'also exist there)')
    parser.add_argument('-n', '--number_of_shards', required=False, type=int,
                        default=1, help='number of shards for data feeder')
    parser.add_argument('-i', '--shard_index', required=False, type=int,
                        default=0, help='shard index')
    parser.add_argument('--use_single_process', dest='use_single_process',
                        action='store_true', default=None,
                        help="use single process for data, prediction and "
                             "callbacks")
    parser.add_argument('--prefetch_buffer_size', required=False, type=int,
                        help='number of batches to prefetch')
    parser.add_argument('--use_tensorrt', dest='use_tensorrt',
                        action='store_true', default=None,
                        help="use tensorrt; tensorrt parameters will be taken "
                             "from inferer.json: tensorrt_config")
    parser.add_argument(
        "-l", "--logLevel", dest="logLevel",
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help="Set logging level")
    return parser


def parse_config_main():
    """
    Parse the arguments and returns the main configuration dict
    if config_main is defined, but other configs are also defined, the values
    from config_main will be overwritten
    """

    parser = construct_parser()
    args = cli_utils.parse_config(parser)
    project_dir = args.pop('project_dir')
    run_name = args.pop('run_name')
    project_dir = os.path.realpath(project_dir)
    os.chdir(project_dir)
    additional_configs_path = args.pop('additional_configs_path', None)
    shard_index = args.get('shard_index')
    number_of_shards = args.get('number_of_shards')
    continue_last = number_of_shards > 1 and shard_index

    config_main = read_inference_configs_from_directories(
        project_dir, run_name=run_name, verify_unused_configs=True,
        additional_dir_with_configs=additional_configs_path,
        continue_last=continue_last)

    config_main['project_dir'] = project_dir
    config_main["batch_size"] = args.get('batch_size')
    config_main["saved_model"] = args.get('saved_model_tag')
    config_main["checkpoint"] = args.get('checkpoint_file_name')
    config_main["number_of_shards"] = number_of_shards
    config_main["shard_index"] = shard_index
    config_main["use_single_process"] = args.get("use_single_process")
    config_main["prefetch_buffer_size"] = args.get("prefetch_buffer_size")
    config_main["use_tensorrt"] = args.get("use_tensorrt")
    config_main["continue_last"] = continue_last
    config_main['run_name'] = run_name
    return config_main


def main():
    """
    Run the inference from the CLI
    """
    _check_number_of_available_gpus()
    config_main = parse_config_main()
    # pylint: disable=missing-kwoa
    # all keywords are inside of config_main
    inferer = build_infer(**config_main)
    inferer.visualize_project_dna(verbosity=0)
    inferer.run()


def _check_number_of_available_gpus():
    available_gpus = tf_utils.get_available_gpus()
    if len(available_gpus) > 1:
        raise ValueError(
            "Currently inference uses only 1 GPU! "
            "You provided {} GPUs. For your help that you can use "
            "other devices for other purposes, please isolate only 1 device "
            "using CUDA_VISIBLE_DEVICES before you start nc7-infer".format(
                len(available_gpus)))


if __name__ == '__main__':
    main()
