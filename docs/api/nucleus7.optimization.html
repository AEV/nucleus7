

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nucleus7.optimization package &mdash; nucleus7 v0.10.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="nucleus7.test_utils package" href="nucleus7.test_utils.html" />
    <link rel="prev" title="nucleus7.model package" href="nucleus7.model.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nucleus7
          

          
            
            <img src="../_static/icon.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.10.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Core.html">Nucleotide and co.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../DataHandling.html">Data handling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Model.html">Model parts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Coordination.html">Coordination</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Optimization.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../KPI.html">KPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Development.html">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Contribution.html">Contribution</a></li>
</ul>
<p class="caption"><span class="caption-text">API:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nucleus7.builders.html">nucleus7.builders package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.coordinator.html">nucleus7.coordinator package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.core.html">nucleus7.core package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.data.html">nucleus7.data package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.kpi.html">nucleus7.kpi package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.model.html">nucleus7.model package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">nucleus7.optimization package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-nucleus7.optimization.configs">nucleus7.optimization.configs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-nucleus7.optimization.learning_rate_manipulator">nucleus7.optimization.learning_rate_manipulator module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-nucleus7.optimization.optimization_handler">nucleus7.optimization.optimization_handler module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-nucleus7.optimization">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.test_utils.html">nucleus7.test_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.third_party.html">nucleus7.third_party package</a></li>
<li class="toctree-l1"><a class="reference internal" href="nucleus7.utils.html">nucleus7.utils package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nucleus7</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>nucleus7.optimization package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/nucleus7.optimization.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nucleus7-optimization-package">
<h1>nucleus7.optimization package<a class="headerlink" href="#nucleus7-optimization-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nucleus7.optimization.configs">
<span id="nucleus7-optimization-configs-module"></span><h2>nucleus7.optimization.configs module<a class="headerlink" href="#module-nucleus7.optimization.configs" title="Permalink to this headline">¶</a></h2>
<p>Configs for optimization</p>
<dl class="class">
<dt id="nucleus7.optimization.configs.OptimizationConfig">
<em class="property">class </em><code class="sig-prename descclassname">nucleus7.optimization.configs.</code><code class="sig-name descname">OptimizationConfig</code><span class="sig-paren">(</span><em class="sig-param">optimizer_parameters</em>, <em class="sig-param">learning_rate</em>, <em class="sig-param">learning_rate_manipulator</em>, <em class="sig-param">gradient_clip</em>, <em class="sig-param">gradient_noise_std</em>, <em class="sig-param">optimizer_name</em>, <em class="sig-param">decouple_regularization</em>, <em class="sig-param">learning_rate_multiplier</em>, <em class="sig-param">optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.__getnewargs__">
<code class="sig-name descname">__getnewargs__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.__getnewargs__" title="Permalink to this definition">¶</a></dt>
<dd><p>Return self as a plain tuple.  Used by copy and pickle.</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.__new__">
<em class="property">static </em><code class="sig-name descname">__new__</code><span class="sig-paren">(</span><em class="sig-param">_cls</em>, <em class="sig-param">optimizer_parameters</em>, <em class="sig-param">learning_rate</em>, <em class="sig-param">learning_rate_manipulator</em>, <em class="sig-param">gradient_clip</em>, <em class="sig-param">gradient_noise_std</em>, <em class="sig-param">optimizer_name</em>, <em class="sig-param">decouple_regularization</em>, <em class="sig-param">learning_rate_multiplier</em>, <em class="sig-param">optimizer</em><span class="sig-paren">)</span><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.__new__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create new instance of OptimizationConfig(optimizer_parameters, learning_rate, learning_rate_manipulator, gradient_clip, gradient_noise_std, optimizer_name, decouple_regularization, learning_rate_multiplier, optimizer)</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.__repr__">
<code class="sig-name descname">__repr__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.__repr__" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a nicely formatted representation string</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.decouple_regularization">
<em class="property">property </em><code class="sig-name descname">decouple_regularization</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.decouple_regularization" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 6</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.gradient_clip">
<em class="property">property </em><code class="sig-name descname">gradient_clip</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.gradient_clip" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.gradient_noise_std">
<em class="property">property </em><code class="sig-name descname">gradient_noise_std</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.gradient_noise_std" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.learning_rate">
<em class="property">property </em><code class="sig-name descname">learning_rate</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.learning_rate_manipulator">
<em class="property">property </em><code class="sig-name descname">learning_rate_manipulator</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.learning_rate_manipulator" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.learning_rate_multiplier">
<em class="property">property </em><code class="sig-name descname">learning_rate_multiplier</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.learning_rate_multiplier" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 7</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.optimizer">
<em class="property">property </em><code class="sig-name descname">optimizer</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 8</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.optimizer_name">
<em class="property">property </em><code class="sig-name descname">optimizer_name</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.optimizer_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.configs.OptimizationConfig.optimizer_parameters">
<em class="property">property </em><code class="sig-name descname">optimizer_parameters</code><a class="headerlink" href="#nucleus7.optimization.configs.OptimizationConfig.optimizer_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nucleus7.optimization.configs.create_and_validate_optimization_config">
<code class="sig-prename descclassname">nucleus7.optimization.configs.</code><code class="sig-name descname">create_and_validate_optimization_config</code><span class="sig-paren">(</span><em class="sig-param">optimizer_name: Optional[str] = None</em>, <em class="sig-param">optimizer_parameters: Optional[dict] = None</em>, <em class="sig-param">learning_rate: Optional[float] = None</em>, <em class="sig-param">learning_rate_manipulator: Optional[nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator] = None</em>, <em class="sig-param">learning_rate_multiplier: Optional[float] = None</em>, <em class="sig-param">gradient_clip: Optional[float] = None</em>, <em class="sig-param">gradient_noise_std: Optional[float] = None</em>, <em class="sig-param">decouple_regularization: bool = False</em>, <em class="sig-param">is_global: bool = False</em>, <em class="sig-param">**deprecated_optimizer_parameters</em><span class="sig-paren">)</span> &#x2192; nucleus7.optimization.configs.OptimizationConfig<a class="reference internal" href="../_modules/nucleus7/optimization/configs.html#create_and_validate_optimization_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.configs.create_and_validate_optimization_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Create configuration for optimization</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer_name</strong> – name of the optimizer to use</p></li>
<li><p><strong>optimizer_parameters</strong> – parameters to provide for the optimizer constructor</p></li>
<li><p><strong>learning_rate</strong> – learning rate to use; in case of use of learning_rate_manipulator,
this will be a initial_learning_rate;
is valid only for global optimization config</p></li>
<li><p><strong>learning_rate_manipulator</strong> – will change the learning rate during the training;
is valid only for global optimization config</p></li>
<li><p><strong>learning_rate_multiplier</strong> – global learning_rate will be multiplied this value and will be used
for this optimizer; is valid only for local optimization configs</p></li>
<li><p><strong>gradient_clip</strong> – will rescale the gradients that the l2 norm of the gradients is not more
than this value; for this method <code class="xref py py-func docutils literal notranslate"><span class="pre">tf.clip_by_global_norm()</span></code> is used</p></li>
<li><p><strong>gradient_noise_std</strong> – will add a normal noise this std to all the gradients in this config</p></li>
<li><p><strong>decouple_regularization</strong> – if the regularization gradients, e.g. gradients from regularization
losses, must be decoupled and use a GradientDescentOptimizer with
current learning rate; otherwise all the gradients will be summed up</p></li>
<li><p><strong>**deprecated_optimizer_parameters</strong> – optimizer_parameters in deprecated form, e.g. if you not provide it
as a optimizer_parameters; is there for backward compatibility</p></li>
<li><p><strong>is_global</strong> – specifies whether this is the global optimization configuration</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>configuration used for optimization</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>optimization config</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.8)"><strong>AssertionError</strong></a> – if learning_rate_multiplier is set for global optimization</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.8)"><strong>AssertionError</strong></a> – if optimizer_name is not set for global optimization</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.8)"><strong>AssertionError</strong></a> – if learning rate is not set for global optimization</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.8)"><strong>AssertionError</strong></a> – if learning_rate_decay is set and is not a dict for global optimization</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.8)"><strong>AssertionError</strong></a> – if learning rate is set for local optimization (is_global == False)</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – if the optimizer parameters were provided inside of the
    optimizer_parameter section and also inside of optimization_parameters
    itself</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nucleus7.optimization.configs.merge_optimization_configs">
<code class="sig-prename descclassname">nucleus7.optimization.configs.</code><code class="sig-name descname">merge_optimization_configs</code><span class="sig-paren">(</span><em class="sig-param">global_optimization_config: nucleus7.optimization.configs.OptimizationConfig</em>, <em class="sig-param">local_optimization_config: nucleus7.optimization.configs.OptimizationConfig</em><span class="sig-paren">)</span> &#x2192; nucleus7.optimization.configs.OptimizationConfig<a class="reference internal" href="../_modules/nucleus7/optimization/configs.html#merge_optimization_configs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.configs.merge_optimization_configs" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an optimization config out of the global and local optimization
configs. All of the values, except of learning_rate* parameters will be
overwritten inside of local_optimization_config from global one if they
were set to None; if optimizer_name is provided, then optimizer_parameters
will be not updated, and if not provided, they will be updated with global,
if parameters are unset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_optimization_config</strong> – global optimization configuration</p></li>
<li><p><strong>local_optimization_config</strong> – local optimization configuration</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>merged optimization configuration</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>local_optimization_config</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-nucleus7.optimization.learning_rate_manipulator">
<span id="nucleus7-optimization-learning-rate-manipulator-module"></span><h2>nucleus7.optimization.learning_rate_manipulator module<a class="headerlink" href="#module-nucleus7.optimization.learning_rate_manipulator" title="Permalink to this headline">¶</a></h2>
<p>Base class for learning rate manipulation</p>
<dl class="class">
<dt id="nucleus7.optimization.learning_rate_manipulator.ConstantLearningRate">
<em class="property">class </em><code class="sig-prename descclassname">nucleus7.optimization.learning_rate_manipulator.</code><code class="sig-name descname">ConstantLearningRate</code><a class="reference internal" href="../_modules/nucleus7/optimization/learning_rate_manipulator.html#ConstantLearningRate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.learning_rate_manipulator.ConstantLearningRate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator" title="nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator"><code class="xref py py-class docutils literal notranslate"><span class="pre">nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator</span></code></a></p>
<p>Class providing a constant learning rate</p>
<dl class="method">
<dt id="nucleus7.optimization.learning_rate_manipulator.ConstantLearningRate.get_current_learning_rate">
<code class="sig-name descname">get_current_learning_rate</code><span class="sig-paren">(</span><em class="sig-param">initial_learning_rate: float</em>, <em class="sig-param">global_step: tensorflow.python.framework.ops.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nucleus7/optimization/learning_rate_manipulator.html#ConstantLearningRate.get_current_learning_rate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.learning_rate_manipulator.ConstantLearningRate.get_current_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>See parent class for documentation</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator">
<em class="property">class </em><code class="sig-prename descclassname">nucleus7.optimization.learning_rate_manipulator.</code><code class="sig-name descname">LearningRateManipulator</code><a class="reference internal" href="../_modules/nucleus7/optimization/learning_rate_manipulator.html#LearningRateManipulator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nucleus7.core.html#nucleus7.core.base.BaseClass" title="nucleus7.core.base.BaseClass"><code class="xref py py-class docutils literal notranslate"><span class="pre">nucleus7.core.base.BaseClass</span></code></a></p>
<p>Base class for learning rate manipulation</p>
<dl class="method">
<dt id="nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator.get_current_learning_rate">
<em class="property">abstract </em><code class="sig-name descname">get_current_learning_rate</code><span class="sig-paren">(</span><em class="sig-param">initial_learning_rate: float</em>, <em class="sig-param">global_step: tensorflow.python.framework.ops.Tensor</em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/nucleus7/optimization/learning_rate_manipulator.html#LearningRateManipulator.get_current_learning_rate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator.get_current_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate and return the current learning rate</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initial_learning_rate</strong> – The specified learning rate</p></li>
<li><p><strong>global_step</strong> – The global step of the optimization</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The currently wanted learning rate</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>current_learning_rate</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nucleus7.optimization.learning_rate_manipulator.TFLearningRateDecay">
<em class="property">class </em><code class="sig-prename descclassname">nucleus7.optimization.learning_rate_manipulator.</code><code class="sig-name descname">TFLearningRateDecay</code><span class="sig-paren">(</span><em class="sig-param">decay_type_name: str</em>, <em class="sig-param">**decay_params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nucleus7/optimization/learning_rate_manipulator.html#TFLearningRateDecay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.learning_rate_manipulator.TFLearningRateDecay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator" title="nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator"><code class="xref py py-class docutils literal notranslate"><span class="pre">nucleus7.optimization.learning_rate_manipulator.LearningRateManipulator</span></code></a></p>
<p>Wraps the learning rate decays from tensorflow itself</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decay_type_name</strong> – name of the decay from tf.nn namespace excluding ‘_decay’ suffix, e.g.
for exponential_decay it should be exponential</p></li>
<li><p><strong>decay_params</strong> – parameters of the decay, which will be passed to decay function</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nucleus7.optimization.learning_rate_manipulator.TFLearningRateDecay.get_current_learning_rate">
<code class="sig-name descname">get_current_learning_rate</code><span class="sig-paren">(</span><em class="sig-param">initial_learning_rate: float</em>, <em class="sig-param">global_step: tensorflow.python.framework.ops.Tensor</em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/nucleus7/optimization/learning_rate_manipulator.html#TFLearningRateDecay.get_current_learning_rate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.learning_rate_manipulator.TFLearningRateDecay.get_current_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>See parent class for documentation</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nucleus7.optimization.optimization_handler">
<span id="nucleus7-optimization-optimization-handler-module"></span><h2>nucleus7.optimization.optimization_handler module<a class="headerlink" href="#module-nucleus7.optimization.optimization_handler" title="Permalink to this headline">¶</a></h2>
<p>Class for OptimizationHandler</p>
<dl class="class">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler">
<em class="property">class </em><code class="sig-prename descclassname">nucleus7.optimization.optimization_handler.</code><code class="sig-name descname">OptimizationHandler</code><a class="reference internal" href="../_modules/nucleus7/optimization/optimization_handler.html#OptimizationHandler"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nucleus7.core.html#nucleus7.core.base.BaseClass" title="nucleus7.core.base.BaseClass"><code class="xref py py-class docutils literal notranslate"><span class="pre">nucleus7.core.base.BaseClass</span></code></a></p>
<p>Main class to create train_op for the training, which allows to use multiple
optimization configurations for different variables</p>
<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.add_config_with_variables">
<code class="sig-name descname">add_config_with_variables</code><span class="sig-paren">(</span><em class="sig-param">config_with_vars: Tuple[nucleus7.optimization.configs.OptimizationConfig, List[tensorflow.python.ops.variables.VariableV1]], name: Optional[str] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nucleus7/optimization/optimization_handler.html#OptimizationHandler.add_config_with_variables"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.add_config_with_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Add tuple (config, variables) of local optimization configs to
variables to use. This variables will be removed from global
optimization</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config_with_vars</strong> – optimization config with list of variables to apply</p></li>
<li><p><strong>name</strong> – needed only to differentiate inside of the warnings / errors</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – if local config has learning_rate or optimizer inside</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – if variables were already used in other configs</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.create_configs_with_grads_and_vars">
<code class="sig-name descname">create_configs_with_grads_and_vars</code><span class="sig-paren">(</span><em class="sig-param">grads_and_vars: List[Tuple[tensorflow.python.framework.ops.Tensor, tensorflow.python.ops.variables.VariableV1]], regularization_grads_and_vars: List[Tuple[tensorflow.python.framework.ops.Tensor, tensorflow.python.ops.variables.VariableV1]], all_trainable_variables: List[tensorflow.python.ops.variables.VariableV1]</em><span class="sig-paren">)</span> &#x2192; List[Tuple[nucleus7.optimization.configs.OptimizationConfig, List[Tuple[tensorflow.python.framework.ops.Tensor, tensorflow.python.ops.variables.VariableV1]]]]<a class="reference internal" href="../_modules/nucleus7/optimization/optimization_handler.html#OptimizationHandler.create_configs_with_grads_and_vars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.create_configs_with_grads_and_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter gradients according to variables used in each config and create
decoupled configs for regularization terms if decouple_regularization
flag is set inside of some config</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>grads_and_vars</strong> – gradients and variables</p></li>
<li><p><strong>regularization_grads_and_vars</strong> – regularization gradients and variables</p></li>
<li><p><strong>all_trainable_variables</strong> – all trainable variables</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of config to variable pairs</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>optim_configs_with_variables</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.filter_grads_and_vars_with_decouple_for_config">
<em class="property">static </em><code class="sig-name descname">filter_grads_and_vars_with_decouple_for_config</code><span class="sig-paren">(</span><em class="sig-param">optim_config</em>, <em class="sig-param">vars_for_config</em>, <em class="sig-param">grads_and_vars</em>, <em class="sig-param">regularization_grads_and_vars</em><span class="sig-paren">)</span> &#x2192; List[Tuple[nucleus7.optimization.configs.OptimizationConfig, List[Tuple[tensorflow.python.framework.ops.Tensor, tensorflow.python.ops.variables.VariableV1]]]]<a class="reference internal" href="../_modules/nucleus7/optimization/optimization_handler.html#OptimizationHandler.filter_grads_and_vars_with_decouple_for_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.filter_grads_and_vars_with_decouple_for_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter gradients and regularization gradients according to variables
of the optimization config and add the decoupled config for the same
variables in case if decouple_regularization flag was provided inside
of optim_config</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optim_config</strong> – optimization config</p></li>
<li><p><strong>vars_for_config</strong> – variables to use with optimization config</p></li>
<li><p><strong>grads_and_vars</strong> – list of (gradient, variable) for all variables in the model</p></li>
<li><p><strong>regularization_grads_and_vars</strong> – list of (gradients, variable) from regularization losses for all
variables in the model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of the same config with filtered gradients and variables with
decoupled config (if it was requested)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>optim_configs_with_variables</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.get_train_op">
<code class="sig-name descname">get_train_op</code><span class="sig-paren">(</span><em class="sig-param">grads_and_vars: List[Tuple[tensorflow.python.framework.ops.Tensor, tensorflow.python.ops.variables.VariableV1]], regularization_grads_and_vars: List[Tuple[tensorflow.python.framework.ops.Tensor, tensorflow.python.ops.variables.VariableV1]], trainable_variables: List[tensorflow.python.ops.variables.VariableV1]</em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Operation<a class="reference internal" href="../_modules/nucleus7/optimization/optimization_handler.html#OptimizationHandler.get_train_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.get_train_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate training operation using optimization configs</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>grads_and_vars</strong> – list of (gradient, variable) for all variables in the model</p></li>
<li><p><strong>regularization_grads_and_vars</strong> – list of (gradients, variable) from regularization losses for all
variables in the model</p></li>
<li><p><strong>trainable_variables</strong> – list of trainable variables</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>train operation with update</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>train_op</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.global_config">
<em class="property">property </em><code class="sig-name descname">global_config</code><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.global_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Global optimization config</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>global optimization config</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>global_config</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.global_learning_rate">
<em class="property">property </em><code class="sig-name descname">global_learning_rate</code><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.global_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Global learning rate</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>global learning rate</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>global_learning_rate</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.global_step">
<em class="property">property </em><code class="sig-name descname">global_step</code><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Current global step</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>global step</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>global_step</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nucleus7.optimization.optimization_handler.OptimizationHandler.initialize_for_session">
<code class="sig-name descname">initialize_for_session</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nucleus7/optimization/optimization_handler.html#OptimizationHandler.initialize_for_session"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nucleus7.optimization.optimization_handler.OptimizationHandler.initialize_for_session" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Perform tensors initialization, e.g.:</dt><dd><ul class="simple">
<li><p>create global step tensor</p></li>
<li><p>create global learning rate</p></li>
<li><p>create all optimizers</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nucleus7.optimization">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nucleus7.optimization" title="Permalink to this headline">¶</a></h2>
<p>Optimization module</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nucleus7.test_utils.html" class="btn btn-neutral float-right" title="nucleus7.test_utils package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="nucleus7.model.html" class="btn btn-neutral float-left" title="nucleus7.model package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Audi Electronics Venture GmbH

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>